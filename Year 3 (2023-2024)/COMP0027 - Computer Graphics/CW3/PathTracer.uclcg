// filename: cw3_student.uclcg
// tabGroup: Coursework
// thumbnail: cw3_thumb.png
// displayname: Coursework 3 - 2023/2024
// shortDescription: Coursework 3 - Path Tracing
// author: None
// isHidden: false

function setup()
{
	UI = {};
	UI.tabs = [];
	UI.titleLong = 'Path Tracer';
	UI.titleShort = 'PathTracer';
	UI.numFrames = 1000;
	UI.maxFPS = 1000;
	UI.renderWidth = 1024;
	UI.renderHeight = 512;

	UI.tabs.push(
		{
		visible: true,
		type: `x-shader/x-fragment`,
		title: `Raytracing`,
		id: `TraceFS`,
		initialValue: `#define SOLUTION_LIGHT
//#define SOLUTION_BOUNCE
//#define SOLUTION_THROUGHPUT
//#define SOLUTION_AA
//#define SOLUTION_MB
//#define SOLUTION_VR

// 0 - cosine weighted hemisphere sampling
// 1 - phong model reflectance sampling - breaks as you keep sampling though
// 2 - halton sampling
const int VR_MODE = 0;

precision highp float;

#define M_PI 3.14159265359

struct Material {
	#ifdef SOLUTION_LIGHT
	vec3 emission;
	float emissionIntensity;
	#endif
	vec3 diffuse;
	vec3 specular;
	float glossiness;
};

struct Sphere {
	vec3 position;
#ifdef SOLUTION_MB
	vec3 motion;
#endif
	float radius;
	Material material;
};

struct Plane {
	vec3 normal;
	float d;
	Material material;
};

const int sphereCount = 4;
const int planeCount = 4;
const int emittingSphereCount = 2;
#ifdef SOLUTION_BOUNCE
const int maxPathLength = 2;
#else
const int maxPathLength = 1;
#endif 

struct Scene {
	Sphere[sphereCount] spheres;
	Plane[planeCount] planes;
};

struct Ray {
	vec3 origin;
	vec3 direction;
};

// Contains all information pertaining to a ray/object intersection
struct HitInfo {
	bool hit;
	float t;
	vec3 position;
	vec3 normal;
	Material material;
};

// Contains info to sample a direction and this directions probability
struct DirectionSample {
	vec3 direction;
	float probability;
};

HitInfo getEmptyHit() {
	Material emptyMaterial;
	#ifdef SOLUTION_LIGHT
	#endif
	emptyMaterial.diffuse = vec3(0.0);
	emptyMaterial.specular = vec3(0.0);
	emptyMaterial.glossiness = 1.0;
	return HitInfo(false, 0.0, vec3(0.0), vec3(0.0), emptyMaterial);
}

// Sorts the two t values such that t1 is smaller than t2
void sortT(inout float t1, inout float t2) {
	// Make t1 the smaller t
	if(t2 < t1)  {
		float temp = t1;
		t1 = t2;
		t2 = temp;
	}
}

// Tests if t is in an interval
bool isTInInterval(const float t, const float tMin, const float tMax) {
	return t > tMin && t < tMax;
}

// Get the smallest t in an interval
bool getSmallestTInInterval(float t0, float t1, const float tMin, const float tMax, inout float smallestTInInterval) {

	sortT(t0, t1);

	// As t0 is smaller, test this first
	if(isTInInterval(t0, tMin, tMax)) {
		smallestTInInterval = t0;
		return true;
	}

	// If t0 was not in the interval, still t1 could be
	if(isTInInterval(t1, tMin, tMax)) {
		smallestTInInterval = t1;
		return true;
	}

	// None was
	return false;
}

// Converts a random integer in 15 bits to a float in (0, 1)
float randomIntegerToRandomFloat(int i) {
	return float(i) / 32768.0;
}

// Returns a random integer for every pixel and dimension that remains the same in all iterations
int pixelIntegerSeed(const int dimensionIndex) {
	vec3 p = vec3(gl_FragCoord.xy, dimensionIndex);
	vec3 r = vec3(23.14069263277926, 2.665144142690225,7.358926345 );
	return int(32768.0 * fract(cos(dot(p,r)) * 123456.0));
}

// Returns a random float for every pixel that remains the same in all iterations
float pixelSeed(const int dimensionIndex) {
	return randomIntegerToRandomFloat(pixelIntegerSeed(dimensionIndex));
}

// The global random seed of this iteration
// It will be set to a new random value in each step
uniform int globalSeed;
int randomSeed;
void initRandomSequence() {
	randomSeed = globalSeed + pixelIntegerSeed(0);
}

// Computes integer  x modulo y not available in most WEBGL SL implementations
int mod(const int x, const int y) {
	return int(float(x) - floor(float(x) / float(y)) * float(y));
}

// Returns the next integer in a pseudo-random sequence
int rand() {
	randomSeed = randomSeed * 1103515245 + 12345;
	return mod(randomSeed / 65536, 32768);
}

float uniformRandomImproved(vec2 co){
    float a = 12.9898;
    float b = 78.233;
    float c = 43758.5453;
    float dt= dot(co.xy ,vec2(a,b));
    float sn= mod(dt,3.14);
    return fract(sin(sn) * c);
}

// Returns the next float in this pixels pseudo-random sequence
float uniformRandom() {
	return randomIntegerToRandomFloat(rand());
}

// This is the index of the sample controlled by the framework.
// It increments by one in every call of this shader
uniform int baseSampleIndex;

// Returns a well-distributed number in (0,1) for the dimension dimensionIndex
float sample(const int dimensionIndex) {
	// combining 2 PRNGs to avoid the patterns in the C-standard LCG
	return uniformRandomImproved(vec2(uniformRandom(), uniformRandom()));
}

// This is a helper function to sample two-dimensionaly in dimension dimensionIndex
vec2 sample2(const int dimensionIndex) {
	return vec2(sample(dimensionIndex + 0), sample(dimensionIndex + 1));
}

vec3 sample3(const int dimensionIndex) {
	return vec3(sample(dimensionIndex + 0), sample(dimensionIndex + 1), sample(dimensionIndex + 2));
}

// This is a register of all dimensions that we will want to sample.
// Thanks to Iliyan Georgiev from Solid Angle for explaining proper housekeeping of sample dimensions in ranomdized Quasi-Monte Carlo
//
// There are infinitely many path sampling dimensions.
// These start at PATH_SAMPLE_DIMENSION.
// The 2D sample pair for vertex i is at PATH_SAMPLE_DIMENSION + PATH_SAMPLE_DIMENSION_MULTIPLIER * i + 0
#define ANTI_ALIAS_SAMPLE_DIMENSION 0
#define TIME_SAMPLE_DIMENSION 1
#define PATH_SAMPLE_DIMENSION 3

// This is 2 for two dimensions and 2 as we use it for two purposese: NEE and path connection
#define PATH_SAMPLE_DIMENSION_MULTIPLIER (2 * 2)

vec3 getEmission(const Material material, const vec3 normal) {
	// In our code the gamma is used in the tonemapping function for gamma correction. Essentially the luminance is 
	// raised to the power of 1 / gamma to encode the image more in line with human perception. Since humans differentiate
	// difference in dark colors better than light colors, applying the gamma encoding, non-linearly scales the color values
	// to match this non-linear perception difference
	#ifdef SOLUTION_LIGHT
	return material.emission * material.emissionIntensity;
	#else
	// This is wrong. It just returns the diffuse color so that you see something to be sure it is working.
	return material.diffuse;
	#endif
}

vec3 getReflectance(const Material material, const vec3 normal, const vec3 inDirection, const vec3 outDirection) {
	#ifdef SOLUTION_THROUGHPUT
	// taken straight from phong equation
	vec3 diffuse_term = material.diffuse / M_PI;
	float normalization_factor = (material.glossiness + 2.0) / (2.0 * M_PI);
	
	// In the phong formula, the reflected ray refers to the reflection of incoming light about the normal. 
	// Since we are shooting rays from camera to light, the incoming light is the same as the negative of the outgoing ray direction
	// which is why we use that in the reflect function
	vec3 reflected_incoming_light = reflect(-outDirection, normal);
	
	// Similarly the phong formula uses the dot product between the outgoing light and the reflected incoming light. Since we are shooting
	// rays from camera to light, the outgoing light is the same as the negative of the incoming ray, so we use that in the
	// dot product instead. There is a max() call here because if the randomly generated outDirection has a too large angle with the normal
	// it could be negative, which is impossible since bdrfs represent the energy of light.
	vec3 specular_term = material.specular * pow(max(0.0, dot(-inDirection, reflected_incoming_light)), material.glossiness);
	
	// summing terms according to phong equation
	return diffuse_term + specular_term * normalization_factor;
	#else
	return vec3(1.0);
	#endif
}

vec3 getGeometricTerm(const Material material, const vec3 normal, const vec3 inDirection, const vec3 outDirection) {
	#ifdef SOLUTION_THROUGHPUT
	// in standard rendering equation the geometric term is the cosine of angle (dot product) between 
	// the negative incoming light direction and surface normal
	
	// Since we are bouncing rays from camera to light, the incoming light in the same as the negative outgoing ray, so we just 
	// use that in the dot product
	return vec3(max(0.0, dot(outDirection, normal)));
	#else
	return vec3(1.0);
	#endif
}

vec3 sphericalToEuclidean(float theta, float phi) {
	float x = sin(theta) * cos(phi);
	float y = sin(theta) * sin(phi);
	float z = cos(theta);
	return vec3(x, y, z);	
}

vec3 getRandomDirection(const int dimensionIndex) {
	#ifdef SOLUTION_BOUNCE
	float uniform_sample_1 = sample(dimensionIndex);
	float uniform_sample_2 = sample(dimensionIndex + 1);
	
	float theta = acos(2.0 * uniform_sample_1 - 1.0);
	float phi = uniform_sample_2 * 2.0 * M_PI;
	
	// A unit test could perhaps be by checking if the sphericalToEuclidean call matches the x, y and z components set manually
	return sphericalToEuclidean(theta, phi);
	
	#else
	// Put your code to compute a random direction in 3D in the #ifdef above
	return vec3(0);
	#endif
}


HitInfo intersectSphere(const Ray ray, Sphere sphere, const float tMin, const float tMax) {

#ifdef SOLUTION_MB
	// Motion blur can be seen as randomly sampling the position between the start and final position of
	// the object. As the path tracer collects samples in this range of positions, it will give the appearance of the
	// object moving between the start and end, since samples and colors are taken over the full range of movement
	
	// In our case, we take the motion of the sphere and scale it by a uniform random number between 0 and 1,
	// and add it to the initial position of the sphere. This generates new positions that will be between the start
	// and end of the sphere's movement determined by the motion variable
	sphere.position += sphere.motion * sample(TIME_SAMPLE_DIMENSION);
#endif
	
	vec3 to_sphere = ray.origin - sphere.position;

	float a = dot(ray.direction, ray.direction);
	float b = 2.0 * dot(ray.direction, to_sphere);
	float c = dot(to_sphere, to_sphere) - sphere.radius * sphere.radius;
	float D = b * b - 4.0 * a * c;
	if (D > 0.0)
	{
		float t0 = (-b - sqrt(D)) / (2.0 * a);
		float t1 = (-b + sqrt(D)) / (2.0 * a);

		float smallestTInInterval;
		if(!getSmallestTInInterval(t0, t1, tMin, tMax, smallestTInInterval)) {
			return getEmptyHit();
		}

		vec3 hitPosition = ray.origin + smallestTInInterval * ray.direction;

		vec3 normal =
			length(ray.origin - sphere.position) < sphere.radius + 0.001?
			-normalize(hitPosition - sphere.position) :
		normalize(hitPosition - sphere.position);

		return HitInfo(
			true,
			smallestTInInterval,
			hitPosition,
			normal,
			sphere.material);
	}
	return getEmptyHit();
}

HitInfo intersectPlane(Ray ray, Plane plane) {
	float t = -(dot(ray.origin, plane.normal) + plane.d) / dot(ray.direction, plane.normal);
	vec3 hitPosition = ray.origin + t * ray.direction;
	return HitInfo(
		true,
		t,
		hitPosition,
		normalize(plane.normal),
		plane.material);
	return getEmptyHit();
}

float lengthSquared(const vec3 x) {
	return dot(x, x);
}

HitInfo intersectScene(Scene scene, Ray ray, const float tMin, const float tMax)
{
	HitInfo best_hit_info;
	best_hit_info.t = tMax;
	best_hit_info.hit = false;

	for (int i = 0; i < sphereCount; ++i) {
		Sphere sphere = scene.spheres[i];
		HitInfo hit_info = intersectSphere(ray, sphere, tMin, tMax);

		if(	hit_info.hit &&
		   hit_info.t < best_hit_info.t &&
		   hit_info.t > tMin)
		{
			best_hit_info = hit_info;
		}
	}

	for (int i = 0; i < planeCount; ++i) {
		Plane plane = scene.planes[i];
		HitInfo hit_info = intersectPlane(ray, plane);

		if(	hit_info.hit &&
		   hit_info.t < best_hit_info.t &&
		   hit_info.t > tMin)
		{
			best_hit_info = hit_info;
		}
	}

	return best_hit_info;
}

mat3 transpose(mat3 m) {
	return mat3(
		m[0][0], m[1][0], m[2][0],
		m[0][1], m[1][1], m[2][1],
		m[0][2], m[1][2], m[2][2]
	);
}

// This function creates a matrix to transform from global space into a local space oriented around the provided vector.
mat3 makeLocalFrame(const vec3 vector) {
	#ifdef SOLUTION_VR
	// Once we have the normal of our hit point, we want to create a new orthonormal basis with the normal as the local z basis vector.
	// To do this, we take the normal vector as the last column of our transformation matrix, and use cross products 
	// to create the new orthonormal x and y basis vectors
	
	// Need a starting vector to generate cross products with. Usually we just use the canonical y-basis.
	// However, if the normal vector is equal to the y basis vector, then the cross product will fail and so we resort to using
	// the canonical x-basis instead.
	vec3 coplanar = vector != vec3(0, 1, 0) ? vec3(0, 1, 0) : vec3(1, 0, 0);
   
	// use cross products to generate orthonormal x and y basis vectors in the local space
	vec3 new_x = normalize(cross(coplanar, vector));
    vec3 new_y = cross(vector, new_x);
	
	// standard transformation matrix with new bases as column vectors
    return mat3(new_x, new_y, vector);
	#else
	return mat3(1.0);
	#endif
}

int primeLookup(const int index) {
	// lookup of first 10 prime numbers
	if (index == 0) return 2;
	if (index == 1) return 3;
	if (index == 2) return 5;
	if (index == 3) return 7;
	if (index == 4) return 11;
	if (index == 5) return 13;
	if (index == 6) return 17;
	if (index == 7) return 19;
	if (index == 8) return 23;
	if (index == 9) return 29;
	return 2;	
}

float halton(const int sampleIndex, const int dimensionIndex) {
	// standard halton function implementation
	float temp = 1.0;
  	float result = 0.0;
	
	int s = sampleIndex;
	int base = primeLookup(dimensionIndex);
	
	for (int i = 0; i < 10; i++) { 
		temp /= float(base);
		result += temp * float(mod(s, base));
		s /= base;
	 }
	
	// use pixelSeed to add random offset to avoid structured artefacts in halton sampling
	 return fract(result + pixelSeed(dimensionIndex));
}

float luminance(const vec3 color) {
    return dot(color, vec3(0.2126, 0.7152, 0.0722));
}

#define EPSILON (1e-6)  // for avoiding numeric issues caused by floating point accuracy

DirectionSample sampleDirection(const vec3 normal, const vec3 inDirection, const vec3 diffuse, const vec3 specular, const float n, const int dimensionIndex) {
	DirectionSample result;
		
	#ifdef SOLUTION_VR
	// Importance sampling will generate vectors on the unit hemisphere. We use the following transformation matrix 
	// convert them to a coordinate system where the z basis vector is aligned with the hit normal vector
	mat3 localFrame = makeLocalFrame(normal);
	
	// uniform samples that are needed to generate samples for importance sampling
	float uniform_sample_1 = sample(dimensionIndex);
    float uniform_sample_2 = sample(dimensionIndex + 1);
	float uniform_sample_3 = sample(dimensionIndex + 2);
	
	if (VR_MODE == 0) {
		// Cosine-weighted hemisphere sampling is a type of importance sampling. Generates points on hemisphere that
		// have probability proportional to cos of the angle with normal vector. Converges quickler since it 
		// approximates geometric term in the rendering equation
		float theta = acos(sqrt(1.0 - uniform_sample_1));
		float phi = 2.0 * M_PI * uniform_sample_2;

		result.direction = localFrame * sphericalToEuclidean(theta, phi);
		// Pdf of cosine weighted sample is just (cos of angle between normal and direction) / pi. To find this we can just use 
		// the dot product of the normal with the direction vector
		result.probability = dot(result.direction, normal) / M_PI;
	}
	
	if (VR_MODE == 1) {
		// BRDF based importance sampling is also a type of importance sampling. In our case it samples from the physically correct phong
		// brdf equation
		
		// Works initially to reduce variance but then starts getting darker. Probably something to do with probability calculation
		// but can't figure it out
		
		float t = length(specular) / (length(specular) + length(diffuse));
		
		// sampling diffuse part of phong equation
		if (uniform_sample_3 > t) {
			float theta = acos(sqrt(uniform_sample_1));
			float phi = 2.0 * M_PI * uniform_sample_2;

			result.direction = localFrame * sphericalToEuclidean(theta, phi);
			result.probability = dot(result.direction, normal) / M_PI;
		}
		
		// sampling specular lobe of phong equation
		else {
			float theta = acos(pow(uniform_sample_1, 1.0 / (n + 1.0)));
			float phi = 2.0 * M_PI * uniform_sample_2;

			vec3 reflected_light = reflect(inDirection, normal);
			// the generated vector is around the reflection vector, so we make a local frame around that
			localFrame = makeLocalFrame(reflected_light);

			result.direction = localFrame * sphericalToEuclidean(theta, phi);
			result.probability = ((n + 1.0) / (2.0 * M_PI)) * pow(max(0.0, dot(result.direction, reflected_light)), n);
		}
		// if the resulting direction is below the hemisphere around hit normal, we want to ignore it
		if (dot(result.direction, normal) < 0.0) {
			result.probability = 1.0;
		}
	}
	
	if (VR_MODE == 2) {
		// Places samples using halton function and is a form of quasi-monte carlo sampling that reduces l2 error of image
		// derives same theta and phi as uniform sampling method, but the samples are taken from halton function instead
		float halton_sample_1 = halton(baseSampleIndex, dimensionIndex);
		float halton_sample_2 = halton(baseSampleIndex, dimensionIndex + 1);
		
		float theta = acos(2.0 * halton_sample_1 - 1.0);
		float phi = halton_sample_2 * 2.0 * M_PI;
		
		result.direction = sphericalToEuclidean(theta, phi);
		// halton is also uniform so pdf is 1 normalized by integral of unit sphere
		result.probability = 1.0 / (4.0 * M_PI);
	}
	#else
	// Depending on the technique: put your variance reduction code in the #ifdef above 
	result.direction = getRandomDirection(dimensionIndex);	
	result.probability = 1.0;
	#endif
	return result;
}

vec3 samplePath(const Scene scene, const Ray initialRay) {

	// Initial result is black
	vec3 result = vec3(0);

	Ray incomingRay = initialRay;
	vec3 throughput = vec3(1.0);
	for(int i = 0; i < maxPathLength; i++) {
		HitInfo hitInfo = intersectScene(scene, incomingRay, 0.001, 10000.0);

		if(!hitInfo.hit) return result;

		result += throughput * getEmission(hitInfo.material, hitInfo.normal);

		Ray outgoingRay;
		DirectionSample directionSample;
		#ifdef SOLUTION_BOUNCE
		directionSample.direction = getRandomDirection(PATH_SAMPLE_DIMENSION + 2 * i);
		
		// Integral of unit sphere is 4pi, so we need to normalize the uniform pdf by this
		directionSample.probability = 1.0 / (4.0 * M_PI);
		
		#ifdef SOLUTION_VR
		// which variance reduction methods is used is controlled by the VR_MODE flag at the top of the code
		directionSample = sampleDirection(hitInfo.normal, incomingRay.direction, hitInfo.material.diffuse, hitInfo.material.specular,
					   hitInfo.material.glossiness, PATH_SAMPLE_DIMENSION + 2 * i);
		#endif
		
		outgoingRay.origin = hitInfo.position + directionSample.direction * 0.001;
		outgoingRay.direction = directionSample.direction;
		#else
			// Put your code to compute the next ray in the #ifdef above
		#endif

		#ifdef SOLUTION_THROUGHPUT
		// Geometric and reflectance terms are multiplied together in the rendering equation integral and so we do the same here.
		// Throughput keeps track of the how the energy of the light changes through bounces, and takes the place of the recursive term inside
		// the integral. Therefore, we multiply the throughout by the geometric and reflectance terms.
		vec3 geometricTerm = getGeometricTerm(hitInfo.material, hitInfo.normal, incomingRay.direction, outgoingRay.direction);
		vec3 reflectanceTerm = getReflectance(hitInfo.material, hitInfo.normal, incomingRay.direction, outgoingRay.direction);
		throughput *= (geometricTerm * reflectanceTerm);
		
		#else
		// Compute the proper throughput in the #ifdef above 
		throughput *= 0.1;
		#endif

		// div by probability of sampled direction 
		throughput /= directionSample.probability; 
	
		#ifdef SOLUTION_BOUNCE
		incomingRay = outgoingRay;
		#else
		// Put some handling of the next and the current ray in the #ifdef above
		#endif
	}
	return result;
}

uniform ivec2 resolution;
Ray getFragCoordRay(const vec2 fragCoord) {

	float sensorDistance = 1.0;
	vec3 origin = vec3(0, 0, sensorDistance);
	vec2 sensorMin = vec2(-1, -0.5);
	vec2 sensorMax = vec2(1, 0.5);
	vec2 pixelSize = (sensorMax - sensorMin) / vec2(resolution);
	vec3 direction = normalize(vec3(sensorMin + pixelSize * fragCoord, -sensorDistance));

	float apertureSize = 0.0;
	float focalPlane = 100.0;
	vec3 sensorPosition = origin + focalPlane * direction;
	origin.xy += -vec2(0.5);
	direction = normalize(sensorPosition - origin);

	return Ray(origin, direction);
}

vec3 colorForFragment(const Scene scene, const vec2 fragCoord) {
	initRandomSequence();

	#ifdef SOLUTION_AA
	// for box filtering in path tracing, we randomly jitter within a pixel to use as the samplecoord.
	// Over multiple samples, these random jitters will generate different paths compared to just using the pixel centre
	// and when averaged out for monte carlo integration, will result in the color of a pixel depending on the average of subpixel samples
	// which is what box filtering is
	
	// to generate the offset, we generate a 2d offset vector, and subtract it by 0.5 to ensure the offset is within -0.5-0.5 (within the pixel)
	vec2 offset = sample2(ANTI_ALIAS_SAMPLE_DIMENSION) - vec2(0.5);
	
	// our sample coord will then be the pixel centre plus the random 2d offset
	vec2 sampleCoord = fragCoord + offset;
	#else  	
	// Put your anti-aliasing code in the #ifdef above
	vec2 sampleCoord = fragCoord;
	#endif
	return samplePath(scene, getFragCoordRay(sampleCoord));
}

void loadScene1(inout Scene scene) {

	scene.spheres[0].position = vec3(7, -2, -12);
	scene.spheres[0].radius = 2.0;
	// Set the value of the missing property in the ifdef below 
#ifdef SOLUTION_LIGHT  
	scene.spheres[0].material.emission = vec3(0.9, 0.9, 0.5);
	scene.spheres[0].material.emissionIntensity = 15.0;
#endif
	scene.spheres[0].material.diffuse = vec3(0.5);
	scene.spheres[0].material.specular = vec3(0.5);
	scene.spheres[0].material.glossiness = 10.0;
	// Set the value of the missing property in the ifdef below 
#ifdef SOLUTION_MB
	scene.spheres[0].motion = vec3(0.0);
#endif
	
	scene.spheres[1].position = vec3(-8, 4, -13);
	scene.spheres[1].radius = 1.0;
	// Set the value of the missing property in the ifdef below 
#ifdef SOLUTION_LIGHT
	scene.spheres[1].material.emission = vec3(0.8, 0.3, 0.1);
	scene.spheres[1].material.emissionIntensity = 15.0;
#endif
	scene.spheres[1].material.diffuse = vec3(0.5);
	scene.spheres[1].material.specular = vec3(0.5);
	scene.spheres[1].material.glossiness = 10.0;
	// Set the value of the missing property in the ifdef below 
#ifdef SOLUTION_MB
	scene.spheres[1].motion = vec3(0.0);
#endif
	
	scene.spheres[2].position = vec3(-2, -2, -12);
	scene.spheres[2].radius = 3.0;
	// Set the value of the missing property in the ifdef below 
#ifdef SOLUTION_LIGHT
	scene.spheres[2].material.emission = vec3(0.0);
	scene.spheres[2].material.emissionIntensity = 0.0;
#endif  
	scene.spheres[2].material.diffuse = vec3(0.2, 0.5, 0.8);
	scene.spheres[2].material.specular = vec3(0.8);
	scene.spheres[2].material.glossiness = 40.0;
	// Set the value of the missing property in the ifdef below 
#ifdef SOLUTION_MB
	scene.spheres[2].motion = vec3(-3.0, 0.0, 3.0);
#endif
	
	scene.spheres[3].position = vec3(3, -3.5, -14);
	scene.spheres[3].radius = 1.0;
	// Set the value of the missing property in the ifdef below 
#ifdef SOLUTION_LIGHT
	scene.spheres[3].material.emission = vec3(0.0);
	scene.spheres[3].material.emissionIntensity = 0.0;
#endif  
	scene.spheres[3].material.diffuse = vec3(0.9, 0.8, 0.8);
	scene.spheres[3].material.specular = vec3(1.0);
	scene.spheres[3].material.glossiness = 10.0;
	// Set the value of the missing property in the ifdef below 
#ifdef SOLUTION_MB
	scene.spheres[3].motion = vec3(2.0, 4.0, 1.0);
#endif
	
	scene.planes[0].normal = vec3(0, 1, 0);
	scene.planes[0].d = 4.5;
	// Set the value of the missing property in the ifdef below 
#ifdef SOLUTION_LIGHT   
	scene.planes[0].material.emission = vec3(0.0);
	scene.planes[0].material.emissionIntensity = 0.0;
#endif
	scene.planes[0].material.diffuse = vec3(0.8);
	scene.planes[0].material.specular = vec3(0.0);
	scene.planes[0].material.glossiness = 50.0;    

	scene.planes[1].normal = vec3(0, 0, 1);
	scene.planes[1].d = 18.5;
	// Set the value of the missing property in the ifdef below 
#ifdef SOLUTION_LIGHT    
	scene.planes[1].material.emission = vec3(0.0);
	scene.planes[1].material.emissionIntensity = 0.0;
#endif
	scene.planes[1].material.diffuse = vec3(0.9, 0.6, 0.3);
	scene.planes[1].material.specular = vec3(0.02);
	scene.planes[1].material.glossiness = 3000.0;

	scene.planes[2].normal = vec3(1, 0,0);
	scene.planes[2].d = 10.0;
	// Set the value of the missing property in the ifdef below 
#ifdef SOLUTION_LIGHT   
	scene.planes[2].material.emission = vec3(0.0);
	scene.planes[2].material.emissionIntensity = 0.0;
#endif
	
	scene.planes[2].material.diffuse = vec3(0.2);
	scene.planes[2].material.specular = vec3(0.1);
	scene.planes[2].material.glossiness = 100.0; 

	scene.planes[3].normal = vec3(-1, 0,0);
	scene.planes[3].d = 10.0;
	// Set the value of the missing property in the ifdef below 
#ifdef SOLUTION_LIGHT    
	scene.planes[3].material.emission = vec3(0.0);
	scene.planes[3].material.emissionIntensity = 0.0;
#endif
	
	scene.planes[3].material.diffuse = vec3(0.2);
	scene.planes[3].material.specular = vec3(0.1);
	scene.planes[3].material.glossiness = 100.0; 
}


void main() {
	// Setup scene
	Scene scene;
	loadScene1(scene);

	// compute color for fragment
	gl_FragColor.rgb = colorForFragment(scene, gl_FragCoord.xy);
	gl_FragColor.a = 1.0;
}
`,
		description: ``,
		wrapFunctionStart: ``,
		wrapFunctionEnd: ``
	});

	UI.tabs.push(
		{
		visible: true,
		type: `x-shader/x-fragment`,
		title: `Tonemapping`,
		id: `CopyFS`,
		initialValue: `precision highp float;

uniform sampler2D radianceTexture;
uniform int sampleCount;
uniform ivec2 resolution;

vec3 tonemap(vec3 color, float maxLuminance, float gamma) {
	float luminance = length(color);
	//float scale =  luminance /  maxLuminance;
	float scale =  luminance / (maxLuminance * luminance + 0.0000001);
  	return max(vec3(0.0), pow(scale * color, vec3(1.0 / gamma)));
}

void main(void) {
  vec3 texel = texture2D(radianceTexture, gl_FragCoord.xy / vec2(resolution)).rgb;
  vec3 radiance = texel / float(sampleCount);
  gl_FragColor.rgb = tonemap(radiance, 1.0, 1.6);
  gl_FragColor.a = 1.0;
}
`,
		description: ``,
		wrapFunctionStart: ``,
		wrapFunctionEnd: ``
	});

	UI.tabs.push(
		{
		visible: false,
		type: `x-shader/x-vertex`,
		title: ``,
		id: `VS`,
		initialValue: `
	attribute vec3 position;
	void main(void) {
		gl_Position = vec4(position, 1.0);
	}
`,
		description: ``,
		wrapFunctionStart: ``,
		wrapFunctionEnd: ``
	});

	 return UI; 
}//!setup


function getShader(gl, id) {

		gl.getExtension('OES_texture_float');
		//alert(gl.getSupportedExtensions());

	var shaderScript = document.getElementById(id);
	if (!shaderScript) {
		return null;
	}

	var str = "";
	var k = shaderScript.firstChild;
	while (k) {
		if (k.nodeType == 3) {
			str += k.textContent;
		}
		k = k.nextSibling;
	}

	var shader;
	if (shaderScript.type == "x-shader/x-fragment") {
		shader = gl.createShader(gl.FRAGMENT_SHADER);
	} else if (shaderScript.type == "x-shader/x-vertex") {
		shader = gl.createShader(gl.VERTEX_SHADER);
	} else {
		return null;
	}

    console.log(str);
	gl.shaderSource(shader, str);
	gl.compileShader(shader);

	if (!gl.getShaderParameter(shader, gl.COMPILE_STATUS)) {
		alert(gl.getShaderInfoLog(shader));
		return null;
	}

	return shader;
}

function RaytracingDemo() {
}

function initShaders() {

	traceProgram = gl.createProgram();
	gl.attachShader(traceProgram, getShader(gl, "VS"));
	gl.attachShader(traceProgram, getShader(gl, "TraceFS"));
	gl.linkProgram(traceProgram);
	gl.useProgram(traceProgram);
	traceProgram.vertexPositionAttribute = gl.getAttribLocation(traceProgram, "position");
	gl.enableVertexAttribArray(traceProgram.vertexPositionAttribute);

	copyProgram = gl.createProgram();
	gl.attachShader(copyProgram, getShader(gl, "VS"));
	gl.attachShader(copyProgram, getShader(gl, "CopyFS"));
	gl.linkProgram(copyProgram);
	gl.useProgram(copyProgram);
	traceProgram.vertexPositionAttribute = gl.getAttribLocation(copyProgram, "position");
	gl.enableVertexAttribArray(copyProgram.vertexPositionAttribute);

}

function initBuffers() {
	triangleVertexPositionBuffer = gl.createBuffer();
	gl.bindBuffer(gl.ARRAY_BUFFER, triangleVertexPositionBuffer);

	var vertices = [
		 -1,  -1,  0,
		 -1,  1,  0,
		 1,  1,  0,

		 -1,  -1,  0,
		 1,  -1,  0,
		 1,  1,  0,
	 ];
	gl.bufferData(gl.ARRAY_BUFFER, new Float32Array(vertices), gl.STATIC_DRAW);
	triangleVertexPositionBuffer.itemSize = 3;
	triangleVertexPositionBuffer.numItems = 3 * 2;
}


function tick() {

// 1st pass: Trace
	gl.bindFramebuffer(gl.FRAMEBUFFER, rttFramebuffer);

	gl.useProgram(traceProgram);
  	gl.uniform1i(gl.getUniformLocation(traceProgram, "globalSeed"), Math.random() * 32768.0);
	gl.uniform1i(gl.getUniformLocation(traceProgram, "baseSampleIndex"), getCurrentFrame());
	gl.uniform2i(
		gl.getUniformLocation(traceProgram, "resolution"),
		getRenderTargetWidth(),
		getRenderTargetHeight());

	gl.bindBuffer(gl.ARRAY_BUFFER, triangleVertexPositionBuffer);
	gl.vertexAttribPointer(
		traceProgram.vertexPositionAttribute,
		triangleVertexPositionBuffer.itemSize,
		gl.FLOAT,
		false,
		0,
		0);

    	gl.viewport(0, 0, gl.viewportWidth, gl.viewportHeight);

	gl.disable(gl.DEPTH_TEST);
	gl.enable(gl.BLEND);
	gl.blendFunc(gl.ONE, gl.ONE);

	gl.drawArrays(gl.TRIANGLES, 0, triangleVertexPositionBuffer.numItems);

// 2nd pass: Average
   	gl.bindFramebuffer(gl.FRAMEBUFFER, null);

	gl.useProgram(copyProgram);
	gl.uniform1i(gl.getUniformLocation(copyProgram, "sampleCount"), getCurrentFrame() + 1);

	gl.bindBuffer(gl.ARRAY_BUFFER, triangleVertexPositionBuffer);
	gl.vertexAttribPointer(
		copyProgram.vertexPositionAttribute,
		triangleVertexPositionBuffer.itemSize,
		gl.FLOAT,
		false,
		0,
		0);

    	gl.viewport(0, 0, gl.viewportWidth, gl.viewportHeight);

	gl.disable(gl.DEPTH_TEST);
	gl.disable(gl.BLEND);

	gl.activeTexture(gl.TEXTURE0);
    	gl.bindTexture(gl.TEXTURE_2D, rttTexture);
	gl.uniform1i(gl.getUniformLocation(copyProgram, "radianceTexture"), 0);
	gl.uniform2i(
		gl.getUniformLocation(copyProgram, "resolution"),
		getRenderTargetWidth(),
		getRenderTargetHeight());

	gl.drawArrays(gl.TRIANGLES, 0, triangleVertexPositionBuffer.numItems);

	gl.bindTexture(gl.TEXTURE_2D, null);
}

function init() {
	initShaders();
	initBuffers();
	gl.clear(gl.COLOR_BUFFER_BIT);

	rttFramebuffer = gl.createFramebuffer();
	gl.bindFramebuffer(gl.FRAMEBUFFER, rttFramebuffer);

	rttTexture = gl.createTexture();
	gl.bindTexture(gl.TEXTURE_2D, rttTexture);
    	gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MAG_FILTER, gl.NEAREST);
    	gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MIN_FILTER, gl.NEAREST);

	gl.texImage2D(gl.TEXTURE_2D, 0, gl.RGBA, getRenderTargetWidth(), getRenderTargetHeight(), 0, gl.RGBA, gl.FLOAT, null);

	gl.framebufferTexture2D(gl.FRAMEBUFFER, gl.COLOR_ATTACHMENT0, gl.TEXTURE_2D, rttTexture, 0);
}

var oldWidth = 0;
var oldTraceProgram;
var oldCopyProgram;
function compute(canvas) {

	if(	getRenderTargetWidth() != oldWidth ||
		oldTraceProgram != document.getElementById("TraceFS") ||
		oldCopyProgram !=  document.getElementById("CopyFS"))
	{
		init();

		oldWidth = getRenderTargetWidth();
		oldTraceProgram = document.getElementById("TraceFS");
		oldCopyProgram = document.getElementById("CopyFS");
	}

	tick();
}
