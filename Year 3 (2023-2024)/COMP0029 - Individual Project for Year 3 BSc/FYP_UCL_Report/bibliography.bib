
@misc{zhang_motiondiffuse_2022,
	title = {{MotionDiffuse}: Text-Driven Human Motion Generation with Diffusion Model},
	url = {http://arxiv.org/abs/2208.15001},
	shorttitle = {{MotionDiffuse}},
	abstract = {Human motion modeling is important for many modern graphics applications, which typically require professional skills. In order to remove the skill barriers for laymen, recent motion generation methods can directly generate human motions conditioned on natural languages. However, it remains challenging to achieve diverse and fine-grained motion generation with various text inputs. To address this problem, we propose {MotionDiffuse}, the first diffusion model-based text-driven motion generation framework, which demonstrates several desired properties over existing methods. 1) Probabilistic Mapping. Instead of a deterministic language-motion mapping, {MotionDiffuse} generates motions through a series of denoising steps in which variations are injected. 2) Realistic Synthesis. {MotionDiffuse} excels at modeling complicated data distribution and generating vivid motion sequences. 3) Multi-Level Manipulation. {MotionDiffuse} responds to fine-grained instructions on body parts, and arbitrary-length motion synthesis with time-varied text prompts. Our experiments show {MotionDiffuse} outperforms existing {SoTA} methods by convincing margins on text-driven motion generation and action-conditioned motion generation. A qualitative analysis further demonstrates {MotionDiffuse}'s controllability for comprehensive motion generation. Homepage: https://mingyuan-zhang.github.io/projects/{MotionDiffuse}.html},
	number = {{arXiv}:2208.15001},
	publisher = {{arXiv}},
	author = {Zhang, Mingyuan and Cai, Zhongang and Pan, Liang and Hong, Fangzhou and Guo, Xinying and Yang, Lei and Liu, Ziwei},
	urldate = {2023-12-03},
	date = {2022-08-31},
	eprinttype = {arxiv},
	eprint = {2208.15001 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/aaryaman/Zotero/storage/4LI3TQUR/2208.html:text/html;Full Text PDF:/Users/aaryaman/Zotero/storage/7TV93QQX/Zhang et al. - 2022 - MotionDiffuse Text-Driven Human Motion Generation.pdf:application/pdf},
}

@article{li_ganimator_2022,
	title = {{GANimator}: neural motion synthesis from a single sequence},
	volume = {41},
	issn = {0730-0301, 1557-7368},
	url = {https://dl.acm.org/doi/10.1145/3528223.3530157},
	doi = {10.1145/3528223.3530157},
	shorttitle = {{GANimator}},
	abstract = {We present {GANimator}, a generative model that learns to synthesize novel motions from a single, short motion sequence. {GANimator} generates motions that resemble the core elements of the original motion, while simultaneously synthesizing novel and diverse movements. Existing data-driven techniques for motion synthesis require a large motion dataset which contains the desired and specific skeletal structure. By contrast, {GANimator} only requires training on a single motion sequence, enabling novel motion synthesis for a variety of skeletal structures e.g., bipeds, quadropeds, hexapeds, and more. Our framework contains a series of generative and adversarial neural networks, each responsible for generating motions in a specific frame rate. The framework progressively learns to synthesize motion from random noise, enabling hierarchical control over the generated motion content across varying levels of detail. We show a number of applications, including crowd simulation, key-frame editing, style transfer, and interactive control, which all learn from a single input sequence. Code and data for this paper are at https://peizhuoli.github.io/ganimator. {CCS} Concepts: • Computing methodologies → Motion processing; Machine learning approaches.},
	pages = {1--12},
	number = {4},
	journaltitle = {{ACM} Transactions on Graphics},
	shortjournal = {{ACM} Trans. Graph.},
	author = {Li, Peizhuo and Aberman, Kfir and Zhang, Zihan and Hanocka, Rana and Sorkine-Hornung, Olga},
	urldate = {2023-12-03},
	date = {2022-07},
	langid = {english},
	file = {Li et al. - 2022 - GANimator neural motion synthesis from a single s.pdf:/Users/aaryaman/Zotero/storage/IBHMD6CB/Li et al. - 2022 - GANimator neural motion synthesis from a single s.pdf:application/pdf},
}

@misc{tevet_human_2022,
	title = {Human Motion Diffusion Model},
	url = {http://arxiv.org/abs/2209.14916},
	abstract = {Natural and expressive human motion generation is the holy grail of computer animation. It is a challenging task, due to the diversity of possible motion, human perceptual sensitivity to it, and the difficulty of accurately describing it. Therefore, current generative solutions are either low-quality or limited in expressiveness. Diffusion models, which have already shown remarkable generative capabilities in other domains, are promising candidates for human motion due to their many-to-many nature, but they tend to be resource hungry and hard to control. In this paper, we introduce Motion Diffusion Model ({MDM}), a carefully adapted classifier-free diffusion-based generative model for the human motion domain. {MDM} is transformer-based, combining insights from motion generation literature. A notable design-choice is the prediction of the sample, rather than the noise, in each diffusion step. This facilitates the use of established geometric losses on the locations and velocities of the motion, such as the foot contact loss. As we demonstrate, {MDM} is a generic approach, enabling different modes of conditioning, and different generation tasks. We show that our model is trained with lightweight resources and yet achieves state-of-the-art results on leading benchmarks for text-to-motion and action-to-motion. https://guytevet.github.io/mdm-page/ .},
	number = {{arXiv}:2209.14916},
	publisher = {{arXiv}},
	author = {Tevet, Guy and Raab, Sigal and Gordon, Brian and Shafir, Yonatan and Cohen-Or, Daniel and Bermano, Amit H.},
	urldate = {2024-01-07},
	date = {2022-10-03},
	eprinttype = {arxiv},
	eprint = {2209.14916 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
	file = {arXiv.org Snapshot:/Users/aaryaman/Zotero/storage/6FAY3NMZ/2209.html:text/html;Full Text PDF:/Users/aaryaman/Zotero/storage/UJ8SQDGF/Tevet et al. - 2022 - Human Motion Diffusion Model.pdf:application/pdf},
}

@misc{nikankin_sinfusion_2023,
	title = {{SinFusion}: Training Diffusion Models on a Single Image or Video},
	url = {http://arxiv.org/abs/2211.11743},
	shorttitle = {{SinFusion}},
	abstract = {Diffusion models exhibited tremendous progress in image and video generation, exceeding {GANs} in quality and diversity. However, they are usually trained on very large datasets and are not naturally adapted to manipulate a given input image or video. In this paper we show how this can be resolved by training a diffusion model on a single input image or video. Our image/video-specific diffusion model ({SinFusion}) learns the appearance and dynamics of the single image or video, while utilizing the conditioning capabilities of diffusion models. It can solve a wide array of image/video-specific manipulation tasks. In particular, our model can learn from few frames the motion and dynamics of a single input video. It can then generate diverse new video samples of the same dynamic scene, extrapolate short videos into long ones (both forward and backward in time) and perform video upsampling. Most of these tasks are not realizable by current video-specific generation methods.},
	number = {{arXiv}:2211.11743},
	publisher = {{arXiv}},
	author = {Nikankin, Yaniv and Haim, Niv and Irani, Michal},
	urldate = {2024-01-07},
	date = {2023-06-19},
	eprinttype = {arxiv},
	eprint = {2211.11743 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/aaryaman/Zotero/storage/2IUAH8T6/2211.html:text/html;Full Text PDF:/Users/aaryaman/Zotero/storage/PH6IHVJ6/Nikankin et al. - 2023 - SinFusion Training Diffusion Models on a Single Image or Video.pdf:application/pdf},
}

@online{raab_single_2023,
	title = {Single Motion Diffusion},
	url = {https://arxiv.org/abs/2302.05905v2},
	abstract = {Synthesizing realistic animations of humans, animals, and even imaginary creatures, has long been a goal for artists and computer graphics professionals. Compared to the imaging domain, which is rich with large available datasets, the number of data instances for the motion domain is limited, particularly for the animation of animals and exotic creatures (e.g., dragons), which have unique skeletons and motion patterns. In this work, we present a Single Motion Diffusion Model, dubbed {SinMDM}, a model designed to learn the internal motifs of a single motion sequence with arbitrary topology and synthesize motions of arbitrary length that are faithful to them. We harness the power of diffusion models and present a denoising network explicitly designed for the task of learning from a single input motion. {SinMDM} is designed to be a lightweight architecture, which avoids overfitting by using a shallow network with local attention layers that narrow the receptive field and encourage motion diversity. {SinMDM} can be applied in various contexts, including spatial and temporal in-betweening, motion expansion, style transfer, and crowd animation. Our results show that {SinMDM} outperforms existing methods both in quality and time-space efficiency. Moreover, while current approaches require additional training for different applications, our work facilitates these applications at inference time. Our code and trained models are available at https://sinmdm.github.io/{SinMDM}-page.},
	titleaddon = {{arXiv}.org},
	author = {Raab, Sigal and Leibovitch, Inbal and Tevet, Guy and Arar, Moab and Bermano, Amit H. and Cohen-Or, Daniel},
	urldate = {2024-01-12},
	date = {2023-02-12},
	langid = {english},
	file = {Full Text PDF:/Users/aaryaman/Zotero/storage/X8G9HI5A/Raab et al. - 2023 - Single Motion Diffusion.pdf:application/pdf},
}

@misc{nichol_improved_2021,
	title = {Improved Denoising Diffusion Probabilistic Models},
	url = {http://arxiv.org/abs/2102.09672},
	abstract = {Denoising diffusion probabilistic models ({DDPM}) are a class of generative models which have recently been shown to produce excellent samples. We show that with a few simple modifications, {DDPMs} can also achieve competitive log-likelihoods while maintaining high sample quality. Additionally, we find that learning variances of the reverse diffusion process allows sampling with an order of magnitude fewer forward passes with a negligible difference in sample quality, which is important for the practical deployment of these models. We additionally use precision and recall to compare how well {DDPMs} and {GANs} cover the target distribution. Finally, we show that the sample quality and likelihood of these models scale smoothly with model capacity and training compute, making them easily scalable. We release our code at https://github.com/openai/improved-diffusion},
	number = {{arXiv}:2102.09672},
	publisher = {{arXiv}},
	author = {Nichol, Alex and Dhariwal, Prafulla},
	urldate = {2024-04-17},
	date = {2021-02-18},
	eprinttype = {arxiv},
	eprint = {2102.09672 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/aaryaman/Zotero/storage/XNCHDB3J/2102.html:text/html;Full Text PDF:/Users/aaryaman/Zotero/storage/GYW9MULM/Nichol and Dhariwal - 2021 - Improved Denoising Diffusion Probabilistic Models.pdf:application/pdf},
}

@inproceedings{li_motion_2002,
	location = {New York, {NY}, {USA}},
	title = {Motion texture: a two-level statistical model for character motion synthesis},
	isbn = {978-1-58113-521-3},
	url = {https://doi.org/10.1145/566570.566604},
	doi = {10.1145/566570.566604},
	series = {{SIGGRAPH} '02},
	shorttitle = {Motion texture},
	abstract = {In this paper, we describe a novel technique, called motion texture, for synthesizing complex human-figure motion (e.g., dancing) that is statistically similar to the original motion captured data. We define motion texture as a set of motion textons and their distribution, which characterize the stochastic and dynamic nature of the captured motion. Specifically, a motion texton is modeled by a linear dynamic system ({LDS}) while the texton distribution is represented by a transition matrix indicating how likely each texton is switched to another. We have designed a maximum likelihood algorithm to learn the motion textons and their relationship from the captured dance motion. The learnt motion texture can then be used to generate new animations automatically and/or edit animation sequences interactively. Most interestingly, motion texture can be manipulated at different levels, either by changing the fine details of a specific motion at the texton level or by designing a new choreography at the distribution level. Our approach is demonstrated by many synthesized sequences of visually compelling dance motion.},
	pages = {465--472},
	booktitle = {Proceedings of the 29th annual conference on Computer graphics and interactive techniques},
	publisher = {Association for Computing Machinery},
	author = {Li, Yan and Wang, Tianshu and Shum, Heung-Yeung},
	urldate = {2024-04-20},
	date = {2002-07-01},
	keywords = {linear dynamic systems, motion editing, motion synthesis, motion texture, texture synthesis},
}

@article{kovar_motion_2002,
	title = {Motion graphs},
	volume = {21},
	issn = {0730-0301},
	url = {https://dl.acm.org/doi/10.1145/566654.566605},
	doi = {10.1145/566654.566605},
	abstract = {In this paper we present a novel method for creating realistic, controllable motion. Given a corpus of motion capture data, we automatically construct a directed graph called a motion graph that encapsulates connections among the database. The motion graph consists both of pieces of original motion and automatically generated transitions. Motion can be generated simply by building walks on the graph. We present a general framework for extracting particular graph walks that meet a user's specifications. We then show how this framework can be applied to the specific problem of generating different styles of locomotion along arbitrary paths.},
	pages = {473--482},
	number = {3},
	journaltitle = {{ACM} Transactions on Graphics},
	shortjournal = {{ACM} Trans. Graph.},
	author = {Kovar, Lucas and Gleicher, Michael and Pighin, Frédéric},
	urldate = {2024-04-20},
	date = {2002-07-01},
	keywords = {animation with constraints, motion capture, motion synthesis},
	file = {Full Text PDF:/Users/aaryaman/Zotero/storage/SNACGXS6/Kovar et al. - 2002 - Motion graphs.pdf:application/pdf},
}

@misc{ho_denoising_2020,
	title = {Denoising Diffusion Probabilistic Models},
	url = {http://arxiv.org/abs/2006.11239},
	doi = {10.48550/arXiv.2006.11239},
	abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional {CIFAR}10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art {FID} score of 3.17. On 256x256 {LSUN}, we obtain sample quality similar to {ProgressiveGAN}. Our implementation is available at https://github.com/hojonathanho/diffusion},
	number = {{arXiv}:2006.11239},
	publisher = {{arXiv}},
	author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
	urldate = {2024-04-20},
	date = {2020-12-16},
	eprinttype = {arxiv},
	eprint = {2006.11239 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/aaryaman/Zotero/storage/U9WZQRKJ/Ho et al. - 2020 - Denoising Diffusion Probabilistic Models.pdf:application/pdf;arXiv.org Snapshot:/Users/aaryaman/Zotero/storage/PIFCS2PV/2006.html:text/html},
}

@misc{goodfellow_generative_2014,
	title = {Generative Adversarial Networks},
	url = {http://arxiv.org/abs/1406.2661},
	doi = {10.48550/arXiv.1406.2661},
	abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
	number = {{arXiv}:1406.2661},
	publisher = {{arXiv}},
	author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	urldate = {2024-04-20},
	date = {2014-06-10},
	eprinttype = {arxiv},
	eprint = {1406.2661 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/aaryaman/Zotero/storage/F4KSTV5M/Goodfellow et al. - 2014 - Generative Adversarial Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/aaryaman/Zotero/storage/69W894A6/1406.html:text/html},
}

@misc{ramesh_zero-shot_2021,
	title = {Zero-Shot Text-to-Image Generation},
	url = {http://arxiv.org/abs/2102.12092},
	doi = {10.48550/arXiv.2102.12092},
	abstract = {Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.},
	number = {{arXiv}:2102.12092},
	publisher = {{arXiv}},
	author = {Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
	urldate = {2024-04-20},
	date = {2021-02-26},
	eprinttype = {arxiv},
	eprint = {2102.12092 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/aaryaman/Zotero/storage/E7BQCRUW/Ramesh et al. - 2021 - Zero-Shot Text-to-Image Generation.pdf:application/pdf;arXiv.org Snapshot:/Users/aaryaman/Zotero/storage/RUEP97LF/2102.html:text/html},
}

@misc{zhou_continuity_2020,
	title = {On the Continuity of Rotation Representations in Neural Networks},
	url = {http://arxiv.org/abs/1812.07035},
	doi = {10.48550/arXiv.1812.07035},
	abstract = {In neural networks, it is often desirable to work with various representations of the same space. For example, 3D rotations can be represented with quaternions or Euler angles. In this paper, we advance a definition of a continuous representation, which can be helpful for training deep neural networks. We relate this to topological concepts such as homeomorphism and embedding. We then investigate what are continuous and discontinuous representations for 2D, 3D, and n-dimensional rotations. We demonstrate that for 3D rotations, all representations are discontinuous in the real Euclidean spaces of four or fewer dimensions. Thus, widely used representations such as quaternions and Euler angles are discontinuous and difficult for neural networks to learn. We show that the 3D rotations have continuous representations in 5D and 6D, which are more suitable for learning. We also present continuous representations for the general case of the n-dimensional rotation group {SO}(n). While our main focus is on rotations, we also show that our constructions apply to other groups such as the orthogonal group and similarity transforms. We finally present empirical results, which show that our continuous rotation representations outperform discontinuous ones for several practical problems in graphics and vision, including a simple autoencoder sanity test, a rotation estimator for 3D point clouds, and an inverse kinematics solver for 3D human poses.},
	number = {{arXiv}:1812.07035},
	publisher = {{arXiv}},
	author = {Zhou, Yi and Barnes, Connelly and Lu, Jingwan and Yang, Jimei and Li, Hao},
	urldate = {2024-04-20},
	date = {2020-06-08},
	eprinttype = {arxiv},
	eprint = {1812.07035 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/aaryaman/Zotero/storage/5K7TMVS7/Zhou et al. - 2020 - On the Continuity of Rotation Representations in Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/aaryaman/Zotero/storage/BJJHDUXW/1812.html:text/html},
}

@article{araujo_computing_2019,
	title = {Computing Receptive Fields of Convolutional Neural Networks},
	volume = {4},
	issn = {2476-0757},
	url = {https://distill.pub/2019/computing-receptive-fields},
	doi = {10.23915/distill.00021},
	abstract = {Detailed derivations and open-source code to analyze the receptive fields of convnets.},
	pages = {e21},
	number = {11},
	journaltitle = {Distill},
	shortjournal = {Distill},
	author = {Araujo, Andr{\textbackslash}\&eacute and Norris, Wade and Sim, Jack},
	urldate = {2024-04-20},
	date = {2019-11-04},
	langid = {english},
	file = {Snapshot:/Users/aaryaman/Zotero/storage/DQ8Z7XJY/computing-receptive-fields.html:text/html},
}

@misc{liu_convnet_2022,
	title = {A {ConvNet} for the 2020s},
	url = {http://arxiv.org/abs/2201.03545},
	doi = {10.48550/arXiv.2201.03545},
	abstract = {The "Roaring 20s" of visual recognition began with the introduction of Vision Transformers ({ViTs}), which quickly superseded {ConvNets} as the state-of-the-art image classification model. A vanilla {ViT}, on the other hand, faces difficulties when applied to general computer vision tasks such as object detection and semantic segmentation. It is the hierarchical Transformers (e.g., Swin Transformers) that reintroduced several {ConvNet} priors, making Transformers practically viable as a generic vision backbone and demonstrating remarkable performance on a wide variety of vision tasks. However, the effectiveness of such hybrid approaches is still largely credited to the intrinsic superiority of Transformers, rather than the inherent inductive biases of convolutions. In this work, we reexamine the design spaces and test the limits of what a pure {ConvNet} can achieve. We gradually "modernize" a standard {ResNet} toward the design of a vision Transformer, and discover several key components that contribute to the performance difference along the way. The outcome of this exploration is a family of pure {ConvNet} models dubbed {ConvNeXt}. Constructed entirely from standard {ConvNet} modules, {ConvNeXts} compete favorably with Transformers in terms of accuracy and scalability, achieving 87.8\% {ImageNet} top-1 accuracy and outperforming Swin Transformers on {COCO} detection and {ADE}20K segmentation, while maintaining the simplicity and efficiency of standard {ConvNets}.},
	number = {{arXiv}:2201.03545},
	publisher = {{arXiv}},
	author = {Liu, Zhuang and Mao, Hanzi and Wu, Chao-Yuan and Feichtenhofer, Christoph and Darrell, Trevor and Xie, Saining},
	urldate = {2024-04-20},
	date = {2022-03-02},
	eprinttype = {arxiv},
	eprint = {2201.03545 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/aaryaman/Zotero/storage/FGTITF37/Liu et al. - 2022 - A ConvNet for the 2020s.pdf:application/pdf;arXiv.org Snapshot:/Users/aaryaman/Zotero/storage/ZQFELHXP/2201.html:text/html},
}

@misc{vaswani_attention_2023,
	title = {Attention Is All You Need},
	url = {http://arxiv.org/abs/1706.03762},
	doi = {10.48550/arXiv.1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 {BLEU} on the {WMT} 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 {BLEU}. On the {WMT} 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art {BLEU} score of 41.8 after training for 3.5 days on eight {GPUs}, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	number = {{arXiv}:1706.03762},
	publisher = {{arXiv}},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	urldate = {2024-04-20},
	date = {2023-08-01},
	eprinttype = {arxiv},
	eprint = {1706.03762 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/aaryaman/Zotero/storage/VFHJ5VZP/Vaswani et al. - 2023 - Attention Is All You Need.pdf:application/pdf;arXiv.org Snapshot:/Users/aaryaman/Zotero/storage/MJHG64AU/1706.html:text/html},
}

@misc{ronneberger_u-net_2015,
	title = {U-Net: Convolutional Networks for Biomedical Image Segmentation},
	url = {http://arxiv.org/abs/1505.04597},
	doi = {10.48550/arXiv.1505.04597},
	shorttitle = {U-Net},
	abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the {ISBI} challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and {DIC}) we won the {ISBI} cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent {GPU}. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
	number = {{arXiv}:1505.04597},
	publisher = {{arXiv}},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	urldate = {2024-04-20},
	date = {2015-05-18},
	eprinttype = {arxiv},
	eprint = {1505.04597 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/aaryaman/Zotero/storage/FILZETWN/Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image Segmentation.pdf:application/pdf;arXiv.org Snapshot:/Users/aaryaman/Zotero/storage/RI2L99RC/1505.html:text/html},
}

@misc{ho_classifier-free_2022,
	title = {Classifier-Free Diffusion Guidance},
	url = {http://arxiv.org/abs/2207.12598},
	doi = {10.48550/arXiv.2207.12598},
	abstract = {Classifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training, in the same spirit as low temperature sampling or truncation in other types of generative models. Classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model. It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance.},
	number = {{arXiv}:2207.12598},
	publisher = {{arXiv}},
	author = {Ho, Jonathan and Salimans, Tim},
	urldate = {2024-04-20},
	date = {2022-07-25},
	eprinttype = {arxiv},
	eprint = {2207.12598 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/aaryaman/Zotero/storage/7P7ZXTQ3/Ho and Salimans - 2022 - Classifier-Free Diffusion Guidance.pdf:application/pdf;arXiv.org Snapshot:/Users/aaryaman/Zotero/storage/T6FBRFZP/2207.html:text/html},
}

@online{benjaminson_reparameterization_nodate,
	title = {The Reparameterization Trick},
	url = {https://sassafras13.github.io/ReparamTrick/},
	abstract = {We first encountered the reparameterization trick when learning about variational autoencoders and how they approximate posterior distributions using {KL} divergence and the Evidence Lower Bound ({ELBO}). We saw that, if we were training a neural network to act as a {VAE}, then eventually we would need to perform backpropagation across a node in the network that was stochastic, because the {ELBO} has a stochastic term. But we cannot perform backpropagation across a stochastic node, because we cannot take the derivative of a random variable. So instead, we used something called the reparameterization trick, to restructure the problem. In this post, I am going to go into this solution in more detail. First I will outline why we need the trick in the first place, and then I will try to provide more insight into the mathematics behind the reparameterization trick.},
	author = {Benjaminson, Emma},
	urldate = {2024-04-20},
	file = {Snapshot:/Users/aaryaman/Zotero/storage/73KJ7I9R/ReparamTrick.html:text/html},
}

@online{truebones_free_2020,
	title = {{FREE} {TRUEBONES} {ZOO}, Over 75 Animals and Animations},
	url = {https://truebones.gumroad.com/p/free-truebones-zoo-over-75-animals-and-animations},
	abstract = {Howdy Folks, Great News! Truebones has just re-released the {FREE} {TRUEBONES} {ZOO}. The Free .{FBX}/.{BVH} {ZOO}. {AMAZING} {COLLECTION} {OF} Over 75 {ANIMATED} {ANIMALS}},
	author = {Truebones},
	urldate = {2024-04-20},
	date = {2020},
	langid = {english},
	file = {Snapshot:/Users/aaryaman/Zotero/storage/TVG293B8/free-truebones-zoo-over-75-animals-and-animations.html:text/html},
}

@article{hicks_evaluation_2022,
	title = {On evaluation metrics for medical applications of artificial intelligence},
	volume = {12},
	issn = {2045-2322},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8993826/},
	doi = {10.1038/s41598-022-09954-8},
	abstract = {Clinicians and software developers need to understand how proposed machine learning ({ML}) models could improve patient care. No single metric captures all the desirable properties of a model, which is why several metrics are typically reported to summarize a model’s performance. Unfortunately, these measures are not easily understandable by many clinicians. Moreover, comparison of models across studies in an objective manner is challenging, and no tool exists to compare models using the same performance metrics. This paper looks at previous {ML} studies done in gastroenterology, provides an explanation of what different metrics mean in the context of binary classification in the presented studies, and gives a thorough explanation of how different metrics should be interpreted. We also release an open source web-based tool that may be used to aid in calculating the most relevant metrics presented in this paper so that other researchers and clinicians may easily incorporate them into their research.},
	pages = {5979},
	journaltitle = {Scientific Reports},
	shortjournal = {Sci Rep},
	author = {Hicks, Steven A. and Strümke, Inga and Thambawita, Vajira and Hammou, Malek and Riegler, Michael A. and Halvorsen, Pål and Parasa, Sravanthi},
	urldate = {2024-04-20},
	date = {2022-04-08},
	pmid = {35395867},
	pmcid = {PMC8993826},
	file = {PubMed Central Full Text PDF:/Users/aaryaman/Zotero/storage/UXJM5AM2/Hicks et al. - 2022 - On evaluation metrics for medical applications of artificial intelligence.pdf:application/pdf},
}

@online{blender_blenderorg_2024,
	title = {blender.org},
	url = {https://www.blender.org/},
	abstract = {The Freedom to Create},
	titleaddon = {blender.org},
	author = {Blender},
	urldate = {2024-04-20},
	date = {2024},
	langid = {english},
	file = {Snapshot:/Users/aaryaman/Zotero/storage/55W7HLKF/www.blender.org.html:text/html},
}
